---
title: "Data Science Methods"
subtitle: "Theory and Application"
author: Linli
format: 
  revealjs:
    slide-level: 3
    smaller: true
    scrollable: true
    incremental: true
    theme: default
    width: 1050
    margin: 0.1
    fig-width: 9
    fig-height: 5
    chalkboard: 
      theme: whiteboard
      boardmarker-width: 5
editor: visual
---

```{r yaml notes}
#globally making all lines in all slides appear increamentally
#format: 
#  revealjs:
#    incremental: true #bullet point appear one by one
#    smaller: true #slide title is smaller
#    scrollable: true #long bullet point can scroll down
```

# The Elements of Statistical Learning: Data Mining, Inference, and Prediction

::: column-margin

------------------------------------------------------------------------

Hastie, Trevor, et al. The elements of statistical learning: data
mining, inference, and prediction. Vol. 2. New York: springer, 2009.
:::

## The Elements of Statistical Learning {auto-animate="true"}

::: nonincremental
-   Supervised learning: predicts (classify or regress) an outcome based
    on some input measures
-   Unsupervised learning: cluster (auto-classify) or association
    (similarity) unlabeled input measures
:::

::: column-margin
The book covered: Linear methods, regularization & smoothing, additive
trees, random forests, neural networks, ensemble learning, and graphical
models
:::

## The Elements of Statistical Learning {auto-animate="true"}

::: nonincremental
::: {.fragment .highlight-blue}
-   Supervised learning: predicts (classify or regress) an outcome based
    on some input measures
:::

-   Unsupervised learning: cluster (auto-classify) or association
    (similarity) unlabeled input measures
:::

# Regression Modeling Strategies

::: column-margin

------------------------------------------------------------------------

Frank, E. H. (2015). Regression modeling strategies with applications to
linear models, logistic and ordinal regression, and survival analysis.
:::

## Why data models?

-   make predictive regression models and let it guide your data
    collection

### 1. Regression \> Hypothesis testing

::: nonincremental
-   Regression (i.e. "predictive modeling") is better than hypothesis
    testing
    -   more than test significance of a statistics, but also estimate
        magnitudes of effects
    -   models may be used to incorporate complex sampling/ measurements
        and conduct many different statistical tests ("Prediction -- a
        superset of hypothesis testing and estimation")
:::

::: {.fragment .highlight-blue}
-   Multivariable regression/modeling is even better: it contains
    important variables and controls them constant so we can estimate
    absolute effects of the variable of interest.
:::

### 2. Prediction \> Classification

-   Prediction model regards outcomes on a **continuum**, while
    classification model forces **dichotomous** outcomes which cause
    information loss [^1]
-   Prediction model may be a necessary first step for building
    classification rules (for a classification model)

[^1]: This also applies to data collection state. We may always use
    continuous variables over categorical ones (which reduces
    measurement errors)



## Choosing Data Models
### Model-guided Data Collection
-   Design data collection with prediction model in mind
    -   stratified sampling (with weights that are representative to the population)
    - cover all important predictors - preferably with their baseline
        measurements
    -   define variables - with reliable and valid measurements verified
    -   specify in-dependency/interaction as well as distribution
        assumption
    -   plans for reducing missing data
    
### Basic principle for choosing data models

-   Choose model based on the **types of outcomes** (ordinal do NOT
    polytomous/multi-nomial model; continuous do NOT logistic model)

-   Develop a model **empirically** (validate different model accuracy
    [^2]) based on the differences between the predicted value (in the
    training set, often bagged [^3] or boosted [^4]) and the observed
    value (in the test set)

-   Understand pros and cons of different types of data models
    (Stochastic vs Algorithmic)

[^2]: i.e. measure generalization error or infinite test set error which
    indicate prediction accuracy

[^3]: bag (bootstrap aggregate) is to "random sample" the training set
    for k times, each time get a f(x) function. For a regression model,
    the bagged predictors are the average over the 1th to kth f(x). For
    a classification model, the bagged predictor is the class that has
    the plurity/most vote from the 1th to kth f(x).

[^4]: instead of random sampling the training set, altering weights on
    he training set to ensemble predictors in a classification model

### Types of models

-   **Parametric/frequentist** (assume probability distribution) vs
    **Non-parametric/ Bayesian** regression models [^5]
    -   Frequentist approach assumes a random/stochastic sample and aims
        to estimate null hypothesis (fixed unknown population parameter)
        based on maximizing the likelihood (p-value, probability to
        reject the bull hypothesis) built on a known population
        distribution.
    -   Bayesian approach is based on the Bayes' theorem: the posterior
        probability (the probability of this hypothesis about a
        population parameter is true after seeing the data) = prior
        probability (a first belief about how likely this hypothesis is
        true) \* the probability of getting the data under the condition
        that the hypothesis is true / probability of getting the data
        regardless of hypothesis is true or false [^6]
-   Accordingly, there are two types of hypothesis testing:
    -   Parametric/rank test assumes sample comes from a certain
        probability distribution and calculates p-value as the
        probability of the occurrence of a given test statistic
    -   Permutation test: randomly samples (without replacement)
        possible permutations and calculates p-value as the proportion
        of samples that have a test statistic more extreme than our
        initial/observed test statistic.
-   Stochastic vs Algorithmic - see the following slides [^7]

[^5]: a kernel is a weighting function used in non-parametric estimation
    techniques.

[^6]: p (hypothesis\|Data)=p(hypothesis)\*p(Data\|hypothesis)/ p(Data)

[^7]: Breiman, L. (2001). Statistical modeling: The two cultures.
    Statistical science, 16(3), 199-231.

### Classic/traditional Regression Model: Stochastic

-   Stochastic (randomized sample with a known x to y function) data
    models
    -   Linear, logistic, and Cox survival analysis [^8]
    -   Can construct simple, understandable x-y relationship, with each
        predictors' importance revealed (by coefficients)
    -   Problem: multiple models could equally well fit the data [^9],
        but they yield different (inaccurate) conclusions.
    - solution: LASSO (least absolute shrinkage and selection operator)  regression analysis method to reduce variable selection ^[penalize absolute values of weight]

[^8]: Poisson regression has count/rate outcome; Cox regression has time
    (between origin and event) outcome based on hazard/fail-risk factors

[^9]: based on goodness-of-fit test and residual plots that give
    arbitrary yes vs no model fit answers. A better way to gauge fit is
    to compare the agreement between predicted with observed y (as long
    as not over-fitting with too many predictors). Cross-validation can
    gauge fit AND avoid the over-fitting problem: 1) Calculate
    validation loss next to training loss. When your validation loss is
    decreasing, the model is still underfit. 2) When your validation
    loss is increasing, the model is overfit.

### Newer Regression Model: Algorithmic/machine-learning

-   Algorithmic (unknown f(x)) data models (i.e. machine learning)
    -   Decision Tree, Neural Networkï¼ŒSupport vector machine [^10]
    -   Application: nonlinear/complex recognition/prediction where
        traditional equation/model can't fit to the data
    -   Can produce more accurate (with more dimensions/multiplicity)
        prediction of y [^11]: by aggregating/en-sembling multiple
        predictors and/or functions/models together. Meanwhile, not
        overfitting (since even though there is no variable deletion, it
        can extract variable importance information from the data) which
        can reduce unstable/different conclusions (i.e. non-accurate
        preduction). [^12]
    -   But usually a black box (i.e. x-y relationship); Although
        Decision Trees have a good interpret-ability based on their
        "nodes" that divide paths to different outcomes, they are not as
        accurate (in term of prediction with a lower low test set error
        in cross-validation) as the less interpret-able Random Forest.

[^10]: support vectors are the data points that have the smallest
    Euclidean metric/distance (in a M-dimension space, the real distance
    of two points or the length of a vector from the original point)
    from the optimal hyperplane (that separating classes in y outcomes).
    "Optimal" here is defined as meaning that the distance of the
    hyperplane to any prediction vector is maximal

[^11]: "Aggregating overa large set of competing models can reduce the
    non-uniqueness while improving accuracy

[^12]: if only measure model fit with test error or
    residual-sum-of-squares, we can easily get different models (and
    after different sets of co-variates get removed, get different
    conclusions)

### Different Ways to Reducing Dimensionality

-   Traditionally way to select/remove variables in a data model: the
    standardized coefficient that a predictor has (i.e. equals to value
    of coefficient/ standard error) decide whether to remove the
    predictor (i.e. importance/ strength of the predictor)
-   Another way to select model: randomly combine different predictors
    to construct model could find multiple subsets that have a lower
    deviance. This can be solved/improved by random forest which
    randomly select variables and calculate the importance of a
    predictor by measuring the noise raise if removing the predictor.

### Big Data (Algorithmic model) Problem

-   Convenient data collection: web survey or extract data from web
    posts

-   Problem: big data representation

    -   Solution: multilevel regression and "borrow predictive strength
        from demographically similar cells that have richer data" [^13]

-   Problem: black box cannot understand the clusters and associations

    -   Solution: cross-validation: build (algorithmic) model with big
        data and verify with a small sample; select a small sample to
        read (human-based) to help understand algorithmic models (and
        maybe also build rules to help iterate algorithmic models)

[^13]: Yu-Sung Su, Reboot the Debate on Social Science Research
    Paradigm, 2022 Peking Education Economics Research Institute
### Example ^[Using Software Development & Machine Learning for Equity in Large-Scale Data Collection & Linkage, 2022 NCES STATS-DC Data Conference] of human-in-the-loop algorithmic models
- a group of human to rate the data sources (i.e. how to use the source for different research/analysis/measurement goals), identify possible important predictors, 
- human suggestions to build model

## Big Names in the field

-   Fisher regression model/specification
-   Classic (Pearson/Neyman) vs Baysian methods
-   Student's

## Linear models

## Logistic models

## Ordinal regression

## Survival analysis

# Hidden slides {visibility="hidden"}

```{r image}
#![](image1.png){.r-stretch}

#![](image2.png){.absolute top=50 right=50 width="450" height="250"}

#![](image3.png){.absolute bottom=0 right=50 width="300" height="300"}
```

```{r sidebysidecols}
#:::: {.columns}

#::: {.column width="50%"}
#- First Left column
#:::

#::: {.column width="50%"}
#- Big Text
#:::

#::::
```

```{r making-notes}
#::: {.notes}
#self notes here: 
#:::

#::: aside
#See more at...
#:::

#some text ^[A footnote]
```

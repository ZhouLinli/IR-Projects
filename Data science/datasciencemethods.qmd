---
title: "Data Science Methods"
subtitle: "Theory and Application"
author: Linli
format: 
  revealjs:
    slide-level: 3
    smaller: true
    scrollable: true
    incremental: true
    theme: default
    width: 1050
    margin: 0.1
    fig-width: 9
    fig-height: 5
    chalkboard: 
      theme: whiteboard
      boardmarker-width: 5
editor: visual
---

```{r yaml notes}
#globally making all lines in all slides appear increamentally
#format: 
#  revealjs:
#    incremental: true #bullet point appear one by one
#    smaller: true #slide title is smaller
#    scrollable: true #long bullet point can scroll down
```

# The Elements of Statistical Learning: Data Mining, Inference, and Prediction

::: column-margin

------------------------------------------------------------------------

Hastie, Trevor, et al. The elements of statistical learning: data
mining, inference, and prediction. Vol. 2. New York: springer, 2009.
:::

## The Elements of Statistical Learning {auto-animate="true"}

::: nonincremental
-   Supervised learning: predicts (classify or regress) an outcome based
    on some input measures
-   Unsupervised learning: cluster (auto-classify) or association
    (similarity) unlabeled input measures
:::

::: column-margin
The book covered: Linear methods, regularization & smoothing, additive
trees, random forests, neural networks, ensemble learning, and graphical
models
:::

## The Elements of Statistical Learning {auto-animate="true"}

::: nonincremental
::: {.fragment .highlight-blue}
-   Supervised learning: predicts (classify or regress) an outcome based
    on some input measures
:::

-   Unsupervised learning: cluster (auto-classify) or association
    (similarity) unlabeled input measures
:::


# Regression Modeling Strategies

::: column-margin

------------------------------------------------------------------------

Frank, E. H. (2015). Regression modeling strategies with applications to
linear models, logistic and ordinal regression, and survival analysis.
:::


## Basic Principles for data modeling
- make predictive regression models and let it guide your data collection
### 1. Regression > Hypothesis testing

::: nonincremental
-   Regression (i.e. "predictive modeling") is better than hypothesis
    testing ^[There are two types of hypothesis testing: 1) Parametric/rank test
that assume sample comes from a certain probability distribution and
calculate p-value as the probability of the occurrence of a given test
statistic; and 2) Permutation test that randomly sample (without
replacement) possible permutations and calculate p-value as the
proportion of samples that have a test statistic more extreme than our
initial/observed test statistic]
    -   more than test significance of a statistics, but also estimate
        magnitudes of effects
    -   models may be used to incorporate complex sampling/ measurements
        and conduct many different statistical tests ("Prediction -- a
        superset of hypothesis testing and estimation")
:::

::: {.fragment .highlight-blue}
-   Multivariable regression/modeling is even better: it contains
    important variables and controls them constant so we can estimate
    absolute effects of the variable of interest.
:::


### 2. Prediction > Classification

-   Prediction model regards outcomes on a **continuum**, while
    classification model forces **dichotomous** outcomes which cause
    information loss [^1]
-   Prediction model may be a necessary first step for building
    classification rules (for a classification model)

[^1]: This also applies to data collection state. We may always use
    continuous variables over categorical ones (which reduces
    measurement errors)

### 3. Model-guided Data Collection

-   Design data collection with prediction model in mind
    -   cover all important predictors - preferably with their baseline
        measurements
    -   define variables - with reliable and valid measurements verified
    -   specify in-dependency/interaction as well as distribution
        assumption
    -   plans for reducing missing data


## Choosing Data Models

-   Develop a model __empirically__ (validate different model accuracy
    between predictions and observations) and based on the __types of outcomes__ (ordinal do NOT polytomous/multi-nomial model; continuous do NOT logistic model)
- Understand pros and cons of different types of data models (Scholastic vs Algorithmic)

### Types of models

-   Parametric (assume probability distribution) vs Non-parametric
    regression models [^2]
- Scholastic vs Algorithmic ^[Breiman, L. (2001). Statistical modeling: The two cultures. Statistical science, 16(3), 199-231.]

[^2]: a kernel is a weighting function used in non-parametric estimation
    techniques. 
    
### Classic/traditional Regression Model: Scholastic

-   Scholastic (randomized sample with a known x to y function) data
    models
    -   linear, logistic, and Cox survival analysis [^3]
    -   can construct simple, understandable x-y relationship, with each
        predictors' importance revealed (by coefficients)
    -   problem: multiple models could equally well fit the data [^4],
        but they yield different (inaccurate) conclusions.

[^3]: Poisson regression has count/rate outcome; Cox regression has time
    (between origin and event) outcome based on hazard/fail-risk factors

[^4]: based on goodness-of-fit test and residual plots that give
    arbitrary yes vs no model fit answers. A better way to gauge fit is
    to compare the agreement between predicted with observed y (as long
    as not over-fitting with too many predictors). Cross-validation can
    gauge fit AND avoid the over-fitting problem: 1) Calculate
    validation loss next to training loss. When your validation loss is
    decreasing, the model is still underfit. 2) When your validation
    loss is increasing, the model is overfit.

### Newer Regression Model: Algorithmic

-   Algorithmic (unknown f(x)) data models (i.e. machine learning)
    -   decision tree, neural network
    -   can produce more accurate (with more dimensions/multiplicity)
        prediction of y [^5]: by aggregating multiple models together
        which can reduce unstable/different conclusions (i.e.
        non-accurate preduction). [^6]
    -   but a black box (i.e. x-y relationship)
    -   application: nonlinear/complex recognition/prediction where
        traditional equation/model can't fit to the data

[^5]: "Aggregating overa large set of competing models can reduce the
    non-uniqueness while improving accuracy

[^6]: if only measure model fit with test error or
    residual-sum-of-squares, we can easily get different models (and
    after different sets of co-variates get removed, get different
    conclusions)

### Big Data (Algorithmic model) Problem

-   Convenient data collection: web survey or extract data from web
    posts

-   Problem: big data representation

    -   Solution: multilevel regression and "borrow predictive strength
        from demographically similar cells that have richer data" [^7]

-   Problem: black box cannot understand the clusters and associations

    -   Solution: cross-validation: build (algorithmic) model with big
        data and verify with a small sample; select a small sample to
        read (human-based) to help understand algorithmic models (and
        maybe also build rules to help iterate algorithmic models)

[^7]: Yu-Sung Su, Reboot the Debate on Social Science Research Paradigm,
    2022 Peking Education Economics Research Institute


## Big Names in the field

-   Fisher regression model/specification
-   Classic (Pearson/Neyman) vs Baysian methods
-   Student's

::: column-margin

------------------------------------------------------------------------

Posterior probability: updating prior probability with a likelihood
function (calculated from applying the Bayes' conditional probability
inference rule)
:::

## Linear models

## Logistic models

## Ordinal regression

## Survival analysis

# Hidden slides {visibility="hidden"}

```{r image}
#![](image1.png){.r-stretch}

#![](image2.png){.absolute top=50 right=50 width="450" height="250"}

#![](image3.png){.absolute bottom=0 right=50 width="300" height="300"}
```

```{r sidebysidecols}
#:::: {.columns}

#::: {.column width="50%"}
#- First Left column
#:::

#::: {.column width="50%"}
#- Big Text
#:::

#::::
```

```{r making-notes}
#::: {.notes}
#self notes here: 
#:::

#::: aside
#See more at...
#:::

#some text ^[A footnote]
```
